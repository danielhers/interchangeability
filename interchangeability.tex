%
% File interchangeability.tex
%
\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{naaclhlt2019}
\usepackage{times}
\usepackage{latexsym}

\usepackage{url}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{pgfplots,pgfplotstable}

\title{Syntactic Interchangeability in Word Embedding Models}

\author{
Daniel Hershcovich \qquad Noam Slonim \\
IBM Research\\
\texttt{\{danielh,noams\}@il.ibm.com}
}

\begin{document}
    \maketitle

    \begin{abstract}
    Nearest neighbors in word embedding models are commonly observed to be
    semantically similar, but the relations between them can vary greatly.
    We investigate the extent to which word embedding models
    preserve syntactic interchangeability, as reflected by distances between
    word vectors, and the effect of hyper-parameters---context window size in particular.
    We use part-of-speech (POS) as a proxy for syntactic interchangeability,
    as generally speaking, words with the same POS are syntactically valid in the same contexts.
    We also investigate the relationship between interchangeability
    and similarity as judged by commonly-used word similarity benchmarks,
    and correlate the result with the performance of word embedding models
    on these benchmarks.
    Our results will inform future research and applications in the selection
    of word embedding model, suggesting a principle for an appropriate selection
    of the context window size parameter depending on the use-case.
    \end{abstract}

    \section{Introduction}\label{sec:introduction}

    Word embedding models \cite{mikolov2013efficient,pennington2014glove,levy2015improving}
    attempt to capture the semantic space of words
    in a metric space of real-valued vectors.
    While it is common knowledge that the hyper-parameters used to train these
    models affects the semantic properties of the distances arising from them
    \cite{goldberg2016primer}, and indeed, it has been shown that
    they capture many different semantic relations \cite{yang2006verb,agirre2009study},
    little has been done to quantify the
    effect of model hyper-parameters on output tendencies.
    In this work, we begin to answer this question.
    As a test case, we experiment with fastText \cite{bojanowski2016enriching}.
    We take the context window size as the hyper-parameter,
    and syntactic interchangeability of words as the relation under investigation,
    expressed by the part-of-speech (POS).
    
    \paragraph{Context window size.}
    Word embedding algorithms learn vector representations for words,
    using co-occurrences from a text corpus,
    based on the distributional hypothesis \cite{harris1954distributional}.
    Co-occurrences are usually extracted by finding, for each word token, all
    words within a constant-size window around that word (or a variably sized
    window up to a certain maximum).
    The size of the context window is a hyper-parameter of the training algorithm,
    referred to as the \textit{context window size}.

    Our experiments\footnote{Our code will be made available upon publication.}
    reveal that context window size is negatively correlated
    with the number of same-POS words among the nearest neighbors of words.
    
    \paragraph{Relation to word similarity.}
    
    Many benchmarks have been proposed for the evaluation of unsupervised word
    representations.
    In general, they can be divided into intrinsic and extrinsic evaluation methods
    \cite{schnabel2015evaluation,chiu2016intrinsic,jastrzebski2017evaluate,alshargi2018concept2vec,bakarov2018survey}.
    While most datasets measure the semantic similarity between words,
    many datasets actually capture semantic relatedness
    \cite{hill2015simlex,avraham2016improving},
    or more complex relations such as analogy or the ability to categorize
    words based on the distributed representation encoded in word embeddings.
    We focus on similarity and relatedness, and evaluate interchangeability
    of related pairs in several common benchmarks (\S\ref{sec:data}).
    
    \section{Experiments}\label{sec:experiments}
    
    \subsection{Data}\label{sec:data}
    We learn word embeddings from a corpus comprising all articles in English Wikipedia,
    using a dump from May 1, 2017\footnote{\url{https://dumps.wikimedia.org/enwiki}}.
    The data is preprocessed using a publicly available preprocessing
    script\footnote{\url{http://mattmahoney.net/dc/textdata.html}},
    extracting text, removing nonalphanumeric characters,
    converting digits to text, and lowercasing the text.
    
    To measure semantic word similarity, we use the following benchmarks:
    \begin{itemize}
        \item MTurk-287~\cite{radinsky2011word}
        \item MTurk-771~\cite{halawi2012large}
        \item WordSim-353~\cite{finkelstein2001placing}
        \item RG65~\cite{rubenstein1965contextual}
        \item Rare Word~\cite{luong2013better}
        \item SimLex999~\cite{hill2015simlex}
        \item WordSim-353-Sim~\cite{agirre2009study}
        \item WordSim-353-Rel~\cite{zesch2008using}
        \item SimVerb-3500~\cite{Gerz2016emnlp}
        \item MEN~\cite{bruni2012distributional}
    \end{itemize}
    
    \subsection{Hyper-parameters}\label{sec:hyperparams}
    We use fastText \cite{bojanowski2016enriching} to learn
    300-dimensional word embedding models,
    using both the SGNS and CBOW algorithms \cite{mikolov2013efficient}.
    The context window size varies from 1 up to 15.
    We set the minimal word occurrence count to 500, to avoid
    very rare words or uncommon spelling errors from skewing the results.
    All other hyper-parameters are set to their default values.
    
    To calculate the similarity between words, we use cosine similarity
    between their corresponding vectors.
    
    \section{Interchangeability in Nearest Neighbor Lists}\label{sec:nn}
    
    As a first experiment, we measure the proportion of same-POS words
    within the list of nearest neighbors for each word in our vocabulary.
    
    \subsection{Collecting Pivots}\label{sec:pivots}
    
    We create word lists for the most
    common parts of speech: nouns, adjectives and verbs.
    For each POS, we list all lemmas of all synsets of that POS from
    WordNet,\footnote{A possible alternative would be to start with the 1K
    vocabulary, use spaCy to tag the POS for each of these unigrams,
    and construct the lists this way.
    However, this would not allow us to remove polysemous words with more
    than one possible POS.}
    and remove from the list any lemma that also belongs to a synset from
    another POS.
    This gives us a list of \textit{uniquely-noun}, \textit{uniquely-adjective}
    and \textit{uniquely-verb} words.
    
    Filtering each of these lists to leave only words that are in
    the 1K vocabulary (see~\S\ref{sec:model}) results in three lists,
    which we refer to as the \textit{pivot lists}.
    They contain 543 nouns, 35 adjectives and 38 verbs, respectively
    
    \subsection{Nearest Neighbor Analysis}\label{sec:nn_analysis}
    
    Using our pre-calculated nearest neighbor lists, we create, for each
    POS, a list of word pairs: for each word in the corresponding pivot list,
    we take its 50 nearest neighbors, and add each one along with the pivot word
    as a (pivot, neighbor) pair.
    
    For each of these lists of pairs, we use spaCy\footnote{\url{https://spacy.io}}
    to tag the POS of the neighbor word in each pair.
    While the spaCy POS-tagger supports tagging word sequences that are longer
    than one word, using it for each of the words separately is likely
    to assign each words its most commonly used POS.
    
    We subsequently calculate a histogram, for each POS $x$, of its
    \textit{neighbor-POS} $y$, that is, the POS assigned to the neighbors of
    words with POS $x$.
    
    \subsection{Results}\label{sec:nn_results}
    
    Figure~\ref{fig:nn_pos_hist} shows the results of this experiment.
    For nouns, adjectives and verbs, we consistently see a decrease in
    the number of same-POS neighbors when we increase the window size,
    while other neighbors are increasing or unaffected.
    
    \pgfplotstableread{
	model	ADJ	ADP	ADV	CCONJ	DET	INTJ	NOUN	NUM	PART	PRON	PROPN	PUNCT	VERB	X
	WikiCBoW300dW03	4841	65	596	7	10	272	19042	24	0	6	89	4	2025	169
	WikiCBoW300dW05	5024	62	664	6	11	290	18658	21	0	8	89	7	2141	169
	WikiCBoW300dW10	5090	69	739	4	10	280	18275	18	0	10	84	6	2374	191
	WikiCBoW300dW15	5160	79	794	3	8	281	18062	14	0	12	85	9	2456	187
	WikiCBoW300dW50	5228	77	853	5	9	264	17817	15	1	14	91	11	2559	206
    }\noun
    \pgfplotstabletranspose[string type,colnames from=model,input colnames to=model]\nounhist{\noun}
    \pgfplotstableread{
	model	ADJ	ADP	ADV	CCONJ	DET	INTJ	NOUN	PRON	PROPN	PUNCT	VERB	X
	WikiCBoW300dW03	924	10	62	0	0	8	666	0	6	1	217	6
	WikiCBoW300dW05	873	10	68	0	0	8	700	0	7	1	224	9
	WikiCBoW300dW10	855	10	74	0	0	8	710	0	6	1	228	8
	WikiCBoW300dW15	828	10	80	0	0	7	722	0	6	2	237	8
	WikiCBoW300dW50	825	10	80	0	0	8	721	1	7	1	236	11
    }\adjective
    \pgfplotstabletranspose[string type,colnames from=model,input colnames to=model]\adjectivehist{\adjective}
    \pgfplotstableread{
	model	ADJ	ADP	ADV	CCONJ	DET	INTJ	NOUN	NUM	PART	PRON	PROPN	PUNCT	VERB	X
	WikiCBoW300dW03	289	3	27	0	0	25	456	0	0	0	2	0	936	12
	WikiCBoW300dW05	305	2	28	0	0	22	462	0	0	0	1	0	921	9
	WikiCBoW300dW10	307	2	36	0	0	21	475	0	0	0	2	0	901	6
	WikiCBoW300dW15	293	4	41	0	0	20	489	0	0	0	3	0	892	8
	WikiCBoW300dW50	307	2	37	0	0	19	474	0	0	0	2	0	903	6
    }\verb
    \pgfplotstabletranspose[string type,colnames from=model,input colnames to=model]\verbhist{\verb}
    \begin{figure*}[ht]
	    \begin{tikzpicture}
		    \begin{axis}[
		    ybar=0pt,  
		    enlarge x limits={0.05},
		    enlarge y limits={value=0.6,upper},
		    ymin=0,
		    width=\textwidth,
		    height=4cm,
		    bar width=4pt,
		    xtick=data,
		    xticklabels from table={\nounhist}{model},
		    xticklabel style={font=\tiny,rotate=45,anchor=north east},
		    xtick align=inside,
		    xticklabel pos=left,
		    tickwidth=0pt,
		    legend image post style={scale=0.5},
		    legend style={at={(axis cs:1,30000)},anchor=north west,font=\tiny},
		    ymajorgrids,
		    ylabel=Neighbors of Nouns,
		    nodes near coords={
		     \tiny\pgfmathprintnumber[precision=0]{\pgfplotspointmeta}
		    },
		    every node near coord/.append style={rotate=90, anchor=west}
		    ]
		    \addplot table[x expr=\coordindex,y=WikiCBoW300dW03]{\nounhist};
		    \addplot table[x expr=\coordindex,y=WikiCBoW300dW05]{\nounhist};
		    \addplot table[x expr=\coordindex,y=WikiCBoW300dW10]{\nounhist};
		    \addplot table[x expr=\coordindex,y=WikiCBoW300dW15]{\nounhist};
		    \addplot table[x expr=\coordindex,y=WikiCBoW300dW50]{\nounhist};
		    \legend{window 3,window 5,window 10,window 15,window 50}
		    \end{axis}
		    \node at (9, 2) {$r=-0.79765$};
		    \node at (9, 1.5) {$p=0.105884$};
	    \end{tikzpicture}
	    \begin{tikzpicture}
		    \begin{axis}[
		    ybar=0pt,
		    enlarge x limits={0.05},
		    enlarge y limits={value=0.3,upper},
		    ymin=0,
		    width=\textwidth,
		    height=4cm,
		    bar width=4pt,
		    xtick=data,
		    xticklabels from table={\adjectivehist}{model},
		    xticklabel style={font=\tiny,rotate=45,anchor=north east},
		    xtick align=inside,
		    xticklabel pos=left,
		    tickwidth=0pt,
		    ymajorgrids,
		    ylabel=Neighbors of Adjectives,
		    nodes near coords={
		     \tiny\pgfmathprintnumber[precision=0]{\pgfplotspointmeta}
		    },
		    every node near coord/.append style={rotate=90, anchor=west}
		    ]
		    \addplot table[x expr=\coordindex,y=WikiCBoW300dW03]{\adjectivehist};
		    \addplot table[x expr=\coordindex,y=WikiCBoW300dW05]{\adjectivehist};
		    \addplot table[x expr=\coordindex,y=WikiCBoW300dW10]{\adjectivehist};
		    \addplot table[x expr=\coordindex,y=WikiCBoW300dW15]{\adjectivehist};
		    \addplot table[x expr=\coordindex,y=WikiCBoW300dW50]{\adjectivehist};
		    \end{axis}
		    \node at (9, 2) {$r=-0.67696$};
		    \node at (9, 1.5) {$p=0.209396$};
	    \end{tikzpicture}
	    \begin{tikzpicture}
		    \begin{axis}[
		    ybar=0pt,  
		    enlarge x limits={0.05},
		    enlarge y limits={value=0.3,upper},
		    ymin=0,
		    width=\textwidth,
		    height=4cm,
		    bar width=4pt,
		    xtick=data,
		    xticklabels from table={\verbhist}{model},
		    xticklabel style={font=\tiny,rotate=45,anchor=north east},
		    xtick align=inside,
		    xticklabel pos=left,
		    tickwidth=0pt,
		    ymajorgrids,
		    ylabel=Neighbors of Verbs,
		    nodes near coords={
		     \tiny\pgfmathprintnumber[precision=0]{\pgfplotspointmeta}
		    },
		    every node near coord/.append style={rotate=90, anchor=west}
		    ]
		    \addplot table[x expr=\coordindex,y=WikiCBoW300dW03]{\verbhist};
		    \addplot table[x expr=\coordindex,y=WikiCBoW300dW05]{\verbhist};
		    \addplot table[x expr=\coordindex,y=WikiCBoW300dW10]{\verbhist};
		    \addplot table[x expr=\coordindex,y=WikiCBoW300dW15]{\verbhist};
		    \addplot table[x expr=\coordindex,y=WikiCBoW300dW50]{\verbhist};
		    \end{axis}
		    \node at (2, 2) {$r=-0.46079$};
		    \node at (2, 1.5) {$p=0.434779$};
	    \end{tikzpicture}
	    \caption{Histogram of neighbor POS for each pivot POS.
	    The number of same-POS neighbors is decreasing with window size,
	    with a Pearson correlation coefficient denoted as $r$
	    and p-value denoted as $p$ (two-tailed t-test).
	    \label{fig:nn_pos_hist}}
	\end{figure*}




    
    
    \section{Interchangeability in Word Pair Benchmarks}\label{sec:benchmarks}


    \subsection{Relatedness Threshold}\label{sec:threshold}
    
    While all of these benchmarks assign a score along a scale to each pair
    (calculated from human scoring), for our experiment we would like to use
    a binary annotation of whether a pair is related or not.
    For this purpose, we divide the whole range of scores,
    for each benchmark, to three parts:
    the lowest 30\% of the range between the lowest and highest scores
    is considered ``unrelated'', the top 30\% as ``related'',
    and the middle 40\% are ignored.
    
    \subsection{Benchmark Analysis}\label{benchmark_analysis}
    
    Given the binary classification obtained from the human-annotated scores
    for each benchmark, we can find the enrichment of same-POS pairs among
    related pairs.
    Again, we use spaCy to annotate the POS for each word in each benchmark
    pair (tagging them in isolation to select the most probably POS),
    and look at the set of same-POS pairs in the benchmark.
    For each of the benchmark, we calculated a p-value using the hypergeometric
    test, comparing the enrichment of same-POS pairs within related pairs,
    with a background of all related and unrelated pairs (ignoring ones in
    the middle 40\% range of scores).
    
    \subsection{Results}\label{sec:benchmark_results}
    
    \begin{table}
    \begin{tabular}{l|c|cc|cc|c}
    \hline
    && \multicolumn{2}{c|}{\bf \# Related} & \multicolumn{2}{c|}{\bf \# Unrelated} & \\
    \bf Benchmark & \bf Size & \bf All & \bf Same-POS & \bf All & \bf Same-POS & \bf p-value \\
    \hline
	WordSim353 & 353 & 122 & 107 & 53 & 40 & 0.038 \\
    WordSim353-S & 203 & 60 & 53 & 53 & 40 & 0.061 \\
    WordSim353-R & 252 & 104 & 89 & 39 & 31 & 0.26 \\
    SimLex999 & 999 & 234 & 199 & 334 & 295 & 0.897 \\
    RW & 2034 & 944 & 555 & 262 & 144 & 0.149 \\
    MEN & 3000 & 791 & 564 & 781 & 439 & $3\cdot10^{-10}$ \\
    MTurk287 & 287 & 49 & 39 & 119 & 68 & 0.004 \\
    MTurk771 & 771 & 204 & 153 & 200 & 146 & 0.365 \\
    SimVerb3500 & 3500 & 633 & 265 & 1217 & 566 & 0.974
    \end{tabular}
    \caption{Analysis of interchangeability (by same-POS) in
    word similarity and relatedness benchmarks.
    \label{tab:benchmark_results}}
    \end{table}
    
    Table~\ref{tab:benchmark_results} shows the results of this experiment.
    For WordSim353, MEN and MTurk287, the set of related pairs
    contains significantly more same-POS pairs than the background set.
    
    In SimLex999 and in SimVerb3500, all pairs are same-POS by design:
    in SimVerb all words are verbs, and in SimLex every pair is of the same POS.
    The fact that not all pairs in these benchmarks are judged as same-POS
    in our experiment is due to ambiguity: for some words, spaCy selected a POS
    which is not the one intended in constructing the benchmark.    

\section{Discussion}\label{sec:discussion}

The same analysis could be applied to the vector dimension.

    \bibliographystyle{acl_natbib}
    \bibliography{references}
\end{document}
